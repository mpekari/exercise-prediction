---
title: Using Classification and Regression Trees(CART) to fit a prediction model for exercise data
author: "Tinniam V Ganesh"
date: "Wednesday, May 20, 2015"
output: html_document
---
# Executive Summary 
This project uses the Classification and Regression Trees (CART)  to model classify the outcome variable based on the set of predictors. The data is read and then cleaned to remove all columns which have >97% of
NAs or space character. Then the CART model is used to classify the outcome based on the predictors.
 

Read the data from the CSV files. Create a vector of NA strings that include both blank spaces and
NAs
```{r}
library(dplyr)
library(caret)
library(rattle)
library(rpart.plot)
training <- read.csv("pml-training.csv",na.strings = c(NA,""))
testing <- read.csv("pml-testing.csv",na.strings = c(NA,""))
```

# Explore the data
```{r}
# Check the column names
str(training)
```

## Clean the data. Remove columns which have a NAs or empty spaces for most rows. Remove  all columns for which colMeans(is.na(training)) < .97

```{r cache=TRUE}
a <- colMeans(is.na(training)) < .97
```

# Subset the training and testing data with the columns for which colMeans > .97
```{r}
train <- training[, a]
test <- testing[,a]
```

# Drop irrelevant columns from training and test
## Drop all columns that don't have relevance to the outcome
```{r cache=TRUE}
# Drop columns that don't have relevance from the training set
train <- select(train, -X)
train <- select(train, -user_name)
train <- select(train, -cvtd_timestamp)
train <-select(train, -new_window)
train <- select(train,-num_window)

# Drop columns that don't have relevance from the testing set
test <- select(test, -X)
test <- select(test, -user_name)
test <- select(test, -cvtd_timestamp)
test <-select(test, -new_window)
test <- select(test,-num_window)


```


# Normalize all the train and test 
Subtract the Mean and divide by the Standard Deviation
``` {r cache=TRUE}
# Training set
value <- length(names(train)) -1
for( i in 1: value ){
  train[,i] <- (train[,i] - mean(train[,i]))/sd(train[,i])
}

# Testing set
value <- length(names(test)) -1
for( i in 1: value ){
  test[,i] <- (test[,i] - mean(test[,i]))/sd(test[,i])
}
```


#  Explore the details of the train set and test set. 
It can be seen that there are 54 predictors and a factor outcome classe
```{r}
dim(train)
dim(test)
```

# Use CART classification &  regression to make the prediction for the training set
```{r cache=TRUE}
modFit <- train(classe ~ ., method="rpart",data= train)
modFit
```

# Plot the classification with plot and fancyRpartPlot
```{r classfication-chart, cache=TRUE}
plot(modFit$finalModel,uniform=TRUE,main='Classification based on exercises') 
text(modFit$finalModel,use.n=TRUE,all=TRUE,cex=0.8)
```

```{r classification-fancy, cache=TRUE}
# Use the fancy Rpart plot for a better visualization of the classification
fancyRpartPlot(modFit$finalModel)
```


# Predict with the test set
```{r cache=TRUE}
predictions <- predict(modFit,newdata=test)
predictions

```

# Conclusion: 
1. The training and the test set data were read from the CSV files
2. The data was cleaned based on the following criteria 
   a. For columns which have blanks remove elements. 
   b. For columns for which there are NA values remove the elements
   c. Remove all irrelevant columns e.g user_name, run_window which don't contribute to the outcome
3. A classification CART model was used for the outcone 'classe' variable
4. After cleaning the total number of predictors is 48
5. A classification chart and a fancy Rpart plot is created
6. The testing set is also cleaned like the training set
7. Finally the predictions for the test set is computed and shown